{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic modules \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# For Model Creation \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, GaussianNoise, GaussianDropout\n",
    "from keras.layers import Flatten, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, SeparableConv2D, AveragePooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras import regularizers, optimizers\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'archive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+'/trainData.csv')\n",
    "\n",
    "image = (path+\"/trainImgs/trainImgs/44e951e45dca.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id_code  diagnosis\n",
      "0  35867_right          3\n",
      "1   4620_right          3\n",
      "2   12063_left          1\n",
      "3    9680_left          4\n",
      "4  16727_right          3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7243, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.head())\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAE/CAYAAACTlB3ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAddElEQVR4nO3df7TcdX3n8edrAyJLLFCxtzFQQ8+iu0BaKncpu116bkpbA1pR22NhqYC2Td3K1p5lT8Xu2dXWsoftStsVu3qisGihRIo/oPxYpbbRbbeoRCkBLDVoPCTSpBgNjbLU6Hv/mG/seO+Ee8nNZ+Zm5vk4Z05mPt/P9/v9vOeTmbzy/X5nJlWFJEmS2vknox6AJEnSuDNwSZIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLklLVpLrkvxWkrOSPDTq8exPkl9P8u5Rj0PS0nXYqAcgSfOpqv8DvGDU49ifqvqvox6DpKXNI1ySJEmNGbgkLRlJfijJp5P8fZL3Ac/s2meSbOvrd3mSh7t+DyZ5ed+yZUmuSvJYki8kuTRJJTmsW74xyVuS/EW3/keSHNe3/kuTPJDkq13ff9G37A1JtnfrPZTk7K79zUmu7+4/M8n1Sb7cbeNTSaaaP3mSljQDl6QlIckzgA8BfwB8N/BHwE/vp/vDwFnA0cBvANcnWdEt+0XgHOA04IXAywas/2+BVwPfAzwD+I/dGJ4P3Aj8KvAc4A7gj5M8I8kLgEuBf1lVzwJeBGwdsO2Lu3GdADwbeC3wxHz1SxpvBi5JS8WZwOHA71XVN6rqZuBTgzpW1R9V1Zeq6ltV9T7gc8AZ3eJXAv+jqrZV1VeAKwds4n9V1d9U1RPATfTCGcDPArdX1V1V9Q3grcCRwL8GvgkcAZyc5PCq2lpVDw/Y9jfoBa1/VlXfrKpNVfX40342JI0VA5ekpeK5wPaqqr62Lw7qmOSiJPd2p+y+CpwK7Dst+Fzgkb7uj8xeH/jbvvtfB5b3rfvtfVbVt7r1V1bVFnpHvt4M7EyyIclzB2z7D4APAxuSfCnJbyc5fFAdkiaHgUvSUvEosDJJ+tq+b3anJM8D3kXv9N6zq+oY4H5g33qPAsf3rXLC0xjDl4Dn9e0r3frbAarqD6vq33R9CvhvszfQHZ37jao6md6RsZcAFz2NMUgaQwYuSUvFXwJ7gV9JcniSV/CPpwn7HUUv7PwdQJJX0zvCtc9NwOuTrExyDPCGpzGGm4AXJzm7Oyp1GfAk8H+TvCDJjyU5Avh/9K7L+tbsDSRZk2R1kmXA4/ROMc7pJ2myGLgkLQlV9Q/AK4BLgF30rqf6wIB+DwJX0QtoO4DVwF/0dXkX8BHgPuAz9C5830vvGqz5xvAQ8HPA1cBjwE8BP9WN7Qh614M9Ru+U5PcAbxywme8FbqYXtj4LfIzeaUZJEyzfebmEJI2XJOcA76yq583bWZIa8QiXpLGS5Mgk5yY5LMlK4E3AB0c9LkmTzSNcksZKkn9K7zTeP6d3ndXtwOv9agZJo2TgkiRJasxTipIkSY0ZuCRJkho7bNQDmM9xxx1Xq1atarqPr33taxx11FFN97FUTXLtMNn1T3LtMNn1W/tk1g6TXf+wat+0adNjVfWc2e1LPnCtWrWKe+65p+k+Nm7cyMzMTNN9LFWTXDtMdv2TXDtMdv3WPjPqYYzMJNc/rNqTDPxJMk8pSpIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLkmSpMYMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktTYkv8txWHYvH03l1x++7z9tl754iGMRpIkjRuPcEmSJDVm4JIkSWrMwCVJktTYvIErybVJdia5v6/tfUnu7W5bk9zbta9K8kTfsnf2rXN6ks1JtiR5W5I0qUiSJGmJWchF89cBbwfeu6+hqn523/0kVwG7+/o/XFWnDdjOO4BfBD4B3AGsBe582iOWJEk6xMx7hKuqPg7sGrSsO0r1SuDGp9pGkhXAd1XV3VVV9MLby572aCVJkg5Bi72G6yxgR1V9rq/txCSfSfKxJGd1bSuBbX19tnVtkiRJYy+9A07zdEpWAbdV1amz2t8BbKmqq7rHRwDLq+rLSU4HPgScAjwfuLKqfrzrdxbwhqp6yX72tw5YBzA1NXX6hg0bDqy6Bdq5azc7npi/3+qVRzcdxyjs2bOH5cuXj3oYIzPJ9U9y7TDZ9Vv7ZNYOk13/sGpfs2bNpqqant1+wF98muQw4BXA6fvaqupJ4Mnu/qYkD9MLW9uB4/tWP75rG6iq1gPrAaanp2tmZuZAh7kgV99wC1dtnv+p2Hph23GMwsaNG2n9/C5lk1z/JNcOk12/tc+MehgjM8n1j7r2xZxS/HHgr6vq26cKkzwnybLu/vcDJwGfr6pHgceTnNld93URcMsi9i1JknTIWMjXQtwI/CXwgiTbkvx8t+h85l4s/6PAfd3XRNwMvLaq9l1w/8vAu4EtwMP4CUVJkjQh5j2PVlUX7Kf9kgFt7wfev5/+9wCnDlomSZI0zvymeUmSpMYMXJIkSY0d8KcUJelQtnn7bi65/PZ5+2298sVDGI2kcecRLkmSpMYMXJIkSY0ZuCRJkhrzGi5NjFUDrte5bPXeOdfxeM2OJOlg8wiXJElSYwYuSZKkxgxckiRJjRm4JEmSGjNwSZIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLkmSpMYMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktSYgUuSJKkxA5ckSVJjBi5JkqTGDFySJEmNGbgkSZIamzdwJbk2yc4k9/e1vTnJ9iT3drdz+5a9McmWJA8leVFf+9qubUuSyw9+KZIkSUvTQo5wXQesHdD+u1V1Wne7AyDJycD5wCndOv8zybIky4DfB84BTgYu6PpKkiSNvcPm61BVH0+yaoHbOw/YUFVPAl9IsgU4o1u2pao+D5BkQ9f3wac/ZEmSpEPLYq7hujTJfd0px2O7tpXAI319tnVt+2uXJEkae6mq+Tv1jnDdVlWndo+ngMeAAt4CrKiq1yR5O3B3VV3f9bsGuLPbzNqq+oWu/VXAD1fVpfvZ3zpgHcDU1NTpGzZsOPAKF2Dnrt3seGL+fqtXHt10HKOwZ88eli9fPuphDMXm7bvntE0dyZy5H8d5HmSS5n4QX/eTOfeTXDtMdv3Dqn3NmjWbqmp6dvu8pxQHqaod++4neRdwW/dwO3BCX9fjuzaeon3Q9tcD6wGmp6drZmbmQIa5YFffcAtXbZ7/qdh6YdtxjMLGjRtp/fwuFZdcfvuctstW750z9+M4z4NM0twP4ut+ZtTDGIlJrh0mu/5R135ApxSTrOh7+HJg3ycYbwXOT3JEkhOBk4BPAp8CTkpyYpJn0Luw/tYDH7YkSdKhY97/3iW5EZgBjkuyDXgTMJPkNHqnFLcCvwRQVQ8kuYnexfB7gddV1Te77VwKfBhYBlxbVQ8c7GIkSZKWooV8SvGCAc3XPEX/K4ArBrTfAdzxtEYnSZI0BvymeUmSpMYMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktSYgUuSJKkxA5ckSVJjBi5JkqTGDFySJEmNGbgkSZIaM3BJkiQ1ZuCSJElqzMAlSZLUmIFLkiSpMQOXJElSYwYuSZKkxgxckiRJjRm4JEmSGjNwSZIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLkmSpMYMXJIkSY0ZuCRJkhqbN3AluTbJziT397X99yR/neS+JB9MckzXvirJE0nu7W7v7Fvn9CSbk2xJ8rYkaVKRJEnSErOQI1zXAWtntd0FnFpVPwD8DfDGvmUPV9Vp3e21fe3vAH4ROKm7zd6mJEnSWJo3cFXVx4Fds9o+UlV7u4d3A8c/1TaSrAC+q6rurqoC3gu87IBGLEmSdIg5GNdwvQa4s+/xiUk+k+RjSc7q2lYC2/r6bOvaJEmSxl56B5zm6ZSsAm6rqlNntf8nYBp4RVVVkiOA5VX15SSnAx8CTgGeD1xZVT/erXcW8Iaqesl+9rcOWAcwNTV1+oYNGw6wvIXZuWs3O56Yv9/qlUc3Hcco7Nmzh+XLl496GEOxefvuOW1TRzJn7sdxngeZpLkfxNf9ZM79JNcOk13/sGpfs2bNpqqant1+2IFuMMklwEuAs7vThFTVk8CT3f1NSR6mF7a2852nHY/v2gaqqvXAeoDp6emamZk50GEuyNU33MJVm+d/KrZe2HYco7Bx40ZaP79LxSWX3z6n7bLVe+fM/TjO8yCTNPeD+LqfGfUwRmKSa4fJrn/UtR/QKcUka4FfA15aVV/va39OkmXd/e+nd3H856vqUeDxJGd2n068CLhl0aOXJEk6BMz737skNwIzwHFJtgFvovepxCOAu7pvd7i7+0TijwK/meQbwLeA11bVvgvuf5neJx6PpHfNV/91X5IkSWNr3sBVVRcMaL5mP33fD7x/P8vuAU4dtEySJGmc+U3zkiRJjRm4JEmSGjNwSZIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLkmSpMYMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktSYgUuSJKkxA5ckSVJjBi5JkqTGDFySJEmNGbgkSZIaM3BJkiQ1ZuCSJElqzMAlSZLU2GGjHoAkSS2suvz2OW2Xrd7LJbPat1754mENSRPMI1ySJEmNGbgkSZIaM3BJkiQ1ZuCSJElqzMAlSZLU2IICV5Jrk+xMcn9f23cnuSvJ57o/j+3ak+RtSbYkuS/JC/vWubjr/7kkFx/8ciRJkpaehR7hug5YO6vtcuCjVXUS8NHuMcA5wEndbR3wDugFNOBNwA8DZwBv2hfSJEmSxtmCAldVfRzYNav5POA93f33AC/ra39v9dwNHJNkBfAi4K6q2lVVXwHuYm6IkyRJGjupqoV1TFYBt1XVqd3jr1bVMd39AF+pqmOS3AZcWVV/3i37KPAGYAZ4ZlX9Vtf+n4EnquqtA/a1jt7RMaampk7fsGHDYmqc185du9nxxPz9Vq88uuk4RmHPnj0sX7581MMYis3bd89pmzqSOXM/jvM8yCTN/SC+7sd/7n3NzzWOcz9ongc58ehlQ6l9zZo1m6pqenb7Qfmm+aqqJAtLbgvb3npgPcD09HTNzMwcrE0PdPUNt3DV5vmfiq0Xth3HKGzcuJHWz+9SMfvbpaH3rdOz534c53mQSZr7QXzdz4x6GM35mp9rHOd+0DwPct3ao0Za+2I+pbijO1VI9+fOrn07cEJfv+O7tv21S5IkjbXFBK5bgX2fNLwYuKWv/aLu04pnArur6lHgw8BPJjm2u1j+J7s2SZKksbagU4pJbqR3DdZxSbbR+7ThlcBNSX4e+CLwyq77HcC5wBbg68CrAapqV5K3AJ/q+v1mVc2+EF+SJGnsLChwVdUF+1l09oC+BbxuP9u5Frh2waOTJEkaA37TvCRJUmMGLkmSpMYMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktSYgUuSJKkxA5ckSVJjBi5JkqTGDFySJEmNGbgkSZIaM3BJkiQ1ZuCSJElqzMAlSZLUmIFLkiSpMQOXJElSYwYuSZKkxgxckiRJjRm4JEmSGjNwSZIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLkmSpMYOOHAleUGSe/tujyf51SRvTrK9r/3cvnXemGRLkoeSvOjglCBJkrS0HXagK1bVQ8BpAEmWAduBDwKvBn63qt7a3z/JycD5wCnAc4E/SfL8qvrmgY5BkiTpUHCwTimeDTxcVV98ij7nARuq6smq+gKwBTjjIO1fkiRpyTpYget84Ma+x5cmuS/JtUmO7dpWAo/09dnWtUmSJI21VNXiNpA8A/gScEpV7UgyBTwGFPAWYEVVvSbJ24G7q+r6br1rgDur6uYB21wHrAOYmpo6fcOGDYsa43x27trNjifm77d65dFNxzEKe/bsYfny5aMexlBs3r57TtvUkcyZ+3Gc50Emae4H8XU//nPva36ucZz7QfM8yIlHLxtK7WvWrNlUVdOz2w/4Gq4+5wCfrqodAPv+BEjyLuC27uF24IS+9Y7v2uaoqvXAeoDp6emamZk5CMPcv6tvuIWrNs//VGy9sO04RmHjxo20fn6Xiksuv31O22Wr986Z+3Gc50Emae4H8XU/M+phNOdrfq5xnPtB8zzIdWuPGmntB+OU4gX0nU5MsqJv2cuB+7v7twLnJzkiyYnAScAnD8L+JUmSlrRFHeFKchTwE8Av9TX/dpLT6J1S3LpvWVU9kOQm4EFgL/A6P6EoSZImwaICV1V9DXj2rLZXPUX/K4ArFrNPSZKkQ43fNC9JktSYgUuSJKkxA5ckSVJjBi5JkqTGDFySJEmNGbgkSZIaM3BJkiQ1ZuCSJElqzMAlSZLUmIFLkiSpMQOXJElSYwYuSZKkxgxckiRJjRm4JEmSGjNwSZIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLkmSpMYMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktSYgUuSJKkxA5ckSVJjiw5cSbYm2Zzk3iT3dG3fneSuJJ/r/jy2a0+StyXZkuS+JC9c7P4lSZKWuoN1hGtNVZ1WVdPd48uBj1bVScBHu8cA5wAndbd1wDsO0v4lSZKWrFanFM8D3tPdfw/wsr7291bP3cAxSVY0GoMkSdKScDACVwEfSbIpybqubaqqHu3u/y0w1d1fCTzSt+62rk2SJGlspaoWt4FkZVVtT/I9wF3Avwdurapj+vp8paqOTXIbcGVV/XnX/lHgDVV1z6xtrqN3ypGpqanTN2zYsKgxzmfnrt3seGL+fqtXHt10HKOwZ88eli9fPuphDMXm7bvntE0dyZy5H8d5HmSS5n4QX/fjP/e+5ucax7kfNM+DnHj0sqHUvmbNmk19l1h922GL3XBVbe/+3Jnkg8AZwI4kK6rq0e6U4c6u+3bghL7Vj+/aZm9zPbAeYHp6umZmZhY7zKd09Q23cNXm+Z+KrRe2HccobNy4kdbP71JxyeW3z2m7bPXeOXM/jvM8yCTN/SC+7mdGPYzmfM3PNY5zP2ieB7lu7VEjrX1RpxSTHJXkWfvuAz8J3A/cClzcdbsYuKW7fytwUfdpxTOB3X2nHiVJksbSYo9wTQEfTLJvW39YVf87yaeAm5L8PPBF4JVd/zuAc4EtwNeBVy9y/5IkSUveogJXVX0e+MEB7V8Gzh7QXsDrFrNPSZKkQ43fNC9JktSYgUuSJKkxA5ckSVJjBi5JkqTGDFySJEmNGbgkSZIaM3BJkiQ1ZuCSJElqzMAlSZLUmIFLkiSpMQOXJElSYwYuSZKkxgxckiRJjRm4JEmSGjNwSZIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLkmSpMYMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktSYgUuSJKkxA5ckSVJjBxy4kpyQ5M+SPJjkgSSv79rfnGR7knu727l967wxyZYkDyV50cEoQJIkaak7bBHr7gUuq6pPJ3kWsCnJXd2y362qt/Z3TnIycD5wCvBc4E+SPL+qvrmIMUiSJC15B3yEq6oerapPd/f/HvgssPIpVjkP2FBVT1bVF4AtwBkHun9JkqRDxUG5hivJKuCHgE90TZcmuS/JtUmO7dpWAo/0rbaNpw5okiRJYyFVtbgNJMuBjwFXVNUHkkwBjwEFvAVYUVWvSfJ24O6qur5b7xrgzqq6ecA21wHrAKampk7fsGHDosY4n527drPjifn7rV55dNNxjMKePXtYvnz5qIcxFJu3757TNnUkc+Z+HOd5kEma+0F83Y//3Puan2sc537QPA9y4tHLhlL7mjVrNlXV9Oz2xVzDRZLDgfcDN1TVBwCqakff8ncBt3UPtwMn9K1+fNc2R1WtB9YDTE9P18zMzGKGOa+rb7iFqzbP/1RsvbDtOEZh48aNtH5+l4pLLr99Tttlq/fOmftxnOdBJmnuB/F1PzPqYTTna36ucZz7QfM8yHVrjxpp7Yv5lGKAa4DPVtXv9LWv6Ov2cuD+7v6twPlJjkhyInAS8MkD3b8kSdKhYjFHuH4EeBWwOcm9XduvAxckOY3eKcWtwC8BVNUDSW4CHqT3CcfX+QlFSZI0CQ44cFXVnwMZsOiOp1jnCuCKA92nJEnSochvmpckSWrMwCVJktSYgUuSJKkxA5ckSVJjBi5JkqTGDFySJEmNGbgkSZIaM3BJkiQ1ZuCSJElqzMAlSZLUmIFLkiSpMQOXJElSYwYuSZKkxgxckiRJjRm4JEmSGjNwSZIkNWbgkiRJaszAJUmS1JiBS5IkqTEDlyRJUmMGLkmSpMYMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktSYgUuSJKmxoQeuJGuTPJRkS5LLh71/SZKkYRtq4EqyDPh94BzgZOCCJCcPcwySJEnDNuwjXGcAW6rq81X1D8AG4Lwhj0GSJGmohh24VgKP9D3e1rVJkiSNrVTV8HaW/Aywtqp+oXv8KuCHq+rSWf3WAeu6hy8AHmo8tOOAxxrvY6ma5Nphsuuf5Nphsuu39sk1yfUPq/bnVdVzZjceNoQd99sOnND3+Piu7TtU1Xpg/bAGleSeqpoe1v6WkkmuHSa7/kmuHSa7fmufzNphsusfde3DPqX4KeCkJCcmeQZwPnDrkMcgSZI0VEM9wlVVe5NcCnwYWAZcW1UPDHMMkiRJwzbsU4pU1R3AHcPe7zyGdvpyCZrk2mGy65/k2mGy67f2yTXJ9Y+09qFeNC9JkjSJ/GkfSZKkxiYqcM33s0JJjkjyvm75J5KsGsEwm1hA7Zck+bsk93a3XxjFOFtIcm2SnUnu38/yJHlb99zcl+SFwx5jKwuofSbJ7r55/y/DHmMrSU5I8mdJHkzyQJLXD+gzznO/kPrHcv6TPDPJJ5P8VVf7bwzoM87v9wupf2zf86H3yzZJPpPktgHLRjP3VTURN3oX6T8MfD/wDOCvgJNn9fll4J3d/fOB94163EOs/RLg7aMea6P6fxR4IXD/fpafC9wJBDgT+MSoxzzE2meA20Y9zka1rwBe2N1/FvA3A/7ej/PcL6T+sZz/bj6Xd/cPBz4BnDmrz1i+3z+N+sf2Pb+r7z8Afzjo7/eo5n6SjnAt5GeFzgPe092/GTg7SYY4xlYm+ieVqurjwK6n6HIe8N7quRs4JsmK4YyurQXUPraq6tGq+nR3/++BzzL3ly3Gee4XUv9Y6uZzT/fw8O42+4LlcX2/X2j9YyvJ8cCLgXfvp8tI5n6SAtdCflbo232qai+wG3j2UEbX1kJ/Uumnu9MqNyc5YcDycTXpPzn1r7pTD3cmOWXUg2mhO2XwQ/T+p99vIub+KeqHMZ3/7pTSvcBO4K6q2u/cj9n7PbCg+mF83/N/D/g14Fv7WT6SuZ+kwKWn9sfAqqr6AeAu/jH9a7x9mt7PUPwgcDXwodEO5+BLshx4P/CrVfX4qMczbPPUP7bzX1XfrKrT6P2iyRlJTh3xkIZqAfWP5Xt+kpcAO6tq06jHMtskBa6F/KzQt/skOQw4GvjyUEbX1ry1V9WXq+rJ7uG7gdOHNLalYEE/OTWOqurxfaceqvcdeYcnOW7EwzpokhxOL2zcUFUfGNBlrOd+vvrHff4BquqrwJ8Ba2ctGtf3+++wv/rH+D3/R4CXJtlK7/KZH0ty/aw+I5n7SQpcC/lZoVuBi7v7PwP8aXVX1R3i5q191nUrL6V3vcekuBW4qPvE2pnA7qp6dNSDGoYk37vv2oUkZ9B7TxiLf3S6uq4BPltVv7OfbmM79wupf1znP8lzkhzT3T8S+Angr2d1G9f3+wXVP67v+VX1xqo6vqpW0fu37k+r6udmdRvJ3A/9m+ZHpfbzs0JJfhO4p6pupffm9AdJttC70Pj80Y344Flg7b+S5KXAXnq1XzKyAR9kSW6k92ms45JsA95E7yJSquqd9H754FxgC/B14NWjGenBt4Dafwb4d0n2Ak8A54/LPzr0/qf7KmBzdy0LwK8D3wfjP/csrP5xnf8VwHuSLKMXIm+qqtsm4f2+s5D6x/Y9f5ClMPd+07wkSVJjk3RKUZIkaSQMXJIkSY0ZuCRJkhozcEmSJDVm4JIkSWrMwCVJktSYgUuSJKkxA5ckSVJj/x+TwtQxlgpchgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.hist(bins=50,figsize=(10,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       3\n",
       "2       1\n",
       "3       4\n",
       "4       3\n",
       "       ..\n",
       "7238    2\n",
       "7239    0\n",
       "7240    2\n",
       "7241    0\n",
       "7242    2\n",
       "Name: diagnosis, Length: 7243, dtype: string"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train['diagnosis'] =  train['diagnosis'].astype(str)\n",
    "train['diagnosis'] =  train['diagnosis'].astype('string')\n",
    "train['id_code'] =  train['id_code'].astype(str)+'.png'\n",
    "X=train['id_code']\n",
    "Y=train['diagnosis']\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5794,) (1449,) (5794,) (1449,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5795 validated image filenames belonging to 5 classes.\n",
      "Found 1448 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 100\n",
    "batch_size = 40\n",
    "directory = path + \"/trainImgs/trainImgs\"\n",
    "common_args = dict(\n",
    "    directory=directory,\n",
    "    x_col=\"id_code\",\n",
    "    y_col=\"diagnosis\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(image_size, image_size))\n",
    "\n",
    "def generate_data_gen(dataframe, subset):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    return datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        subset=subset,\n",
    "        **common_args)\n",
    "\n",
    "\n",
    "train_gen = generate_data_gen(train, 'training')\n",
    "test_gen = generate_data_gen(train, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train['diagnosis']\n",
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "num_classes = y_train.shape[1]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    #model.add(Reshape((x_train.shape[0],),))\n",
    "    #model.add(GaussianDropout(0.3,input_shape=[96,96,3]))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (100,100,3)))\n",
    "    model.add(GaussianDropout(0.3))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(96, (5, 5), activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.0001)\n",
    "                   ,activity_regularizer=regularizers.l1(0.01)))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.0001, amsgrad=True), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 100, 100, 32)      2432      \n",
      "                                                                 \n",
      " gaussian_dropout (GaussianD  (None, 100, 100, 32)     0         \n",
      " ropout)                                                         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 96, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 46, 46, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 23, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 19, 19, 96)        153696    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 19, 19, 96)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 34656)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               4436096   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,687,121\n",
      "Trainable params: 4,687,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rameshwari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "es= EarlyStopping(monitor='val_loss', mode ='min', verbose = 1, patience = 20)\n",
    "mc = ModelCheckpoint('blindness.model', monitor='val_loss', save_best_only = True, mode ='min', verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rameshwari\\AppData\\Local\\Temp\\ipykernel_27724\\2130852465.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model22=model.fit_generator(generator=train_gen,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 2.5205 - accuracy: 0.3287\n",
      "Epoch 1: val_loss improved from inf to 2.03400, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 44s 275ms/step - loss: 2.5205 - accuracy: 0.3287 - val_loss: 2.0340 - val_accuracy: 0.1222\n",
      "Epoch 2/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.6766 - accuracy: 0.3750\n",
      "Epoch 2: val_loss improved from 2.03400 to 1.78908, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 37s 250ms/step - loss: 1.6766 - accuracy: 0.3750 - val_loss: 1.7891 - val_accuracy: 0.2307\n",
      "Epoch 3/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.5329 - accuracy: 0.4169\n",
      "Epoch 3: val_loss improved from 1.78908 to 1.72326, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 36s 246ms/step - loss: 1.5329 - accuracy: 0.4169 - val_loss: 1.7233 - val_accuracy: 0.2631\n",
      "Epoch 4/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.4410 - accuracy: 0.4528\n",
      "Epoch 4: val_loss improved from 1.72326 to 1.69252, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 40s 271ms/step - loss: 1.4410 - accuracy: 0.4528 - val_loss: 1.6925 - val_accuracy: 0.2762\n",
      "Epoch 5/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.3854 - accuracy: 0.4806\n",
      "Epoch 5: val_loss improved from 1.69252 to 1.62408, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 37s 255ms/step - loss: 1.3854 - accuracy: 0.4806 - val_loss: 1.6241 - val_accuracy: 0.2970\n",
      "Epoch 6/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.3373 - accuracy: 0.4985\n",
      "Epoch 6: val_loss improved from 1.62408 to 1.56527, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 36s 247ms/step - loss: 1.3373 - accuracy: 0.4985 - val_loss: 1.5653 - val_accuracy: 0.2714\n",
      "Epoch 7/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.2970 - accuracy: 0.5073\n",
      "Epoch 7: val_loss improved from 1.56527 to 1.54447, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 39s 265ms/step - loss: 1.2970 - accuracy: 0.5073 - val_loss: 1.5445 - val_accuracy: 0.3356\n",
      "Epoch 8/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.2715 - accuracy: 0.5215\n",
      "Epoch 8: val_loss did not improve from 1.54447\n",
      "145/145 [==============================] - 33s 225ms/step - loss: 1.2715 - accuracy: 0.5215 - val_loss: 1.6941 - val_accuracy: 0.3080\n",
      "Epoch 9/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.2432 - accuracy: 0.5399\n",
      "Epoch 9: val_loss did not improve from 1.54447\n",
      "145/145 [==============================] - 32s 222ms/step - loss: 1.2432 - accuracy: 0.5399 - val_loss: 1.6629 - val_accuracy: 0.2928\n",
      "Epoch 10/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.2131 - accuracy: 0.5425\n",
      "Epoch 10: val_loss improved from 1.54447 to 1.53162, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 37s 256ms/step - loss: 1.2131 - accuracy: 0.5425 - val_loss: 1.5316 - val_accuracy: 0.3211\n",
      "Epoch 11/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.2046 - accuracy: 0.5460\n",
      "Epoch 11: val_loss did not improve from 1.53162\n",
      "145/145 [==============================] - 37s 252ms/step - loss: 1.2046 - accuracy: 0.5460 - val_loss: 1.5939 - val_accuracy: 0.3467\n",
      "Epoch 12/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.1599 - accuracy: 0.5710\n",
      "Epoch 12: val_loss improved from 1.53162 to 1.52164, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 41s 279ms/step - loss: 1.1599 - accuracy: 0.5710 - val_loss: 1.5216 - val_accuracy: 0.3488\n",
      "Epoch 13/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.1446 - accuracy: 0.5824\n",
      "Epoch 13: val_loss did not improve from 1.52164\n",
      "145/145 [==============================] - 38s 257ms/step - loss: 1.1446 - accuracy: 0.5824 - val_loss: 1.6054 - val_accuracy: 0.3391\n",
      "Epoch 14/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.1258 - accuracy: 0.5874\n",
      "Epoch 14: val_loss did not improve from 1.52164\n",
      "145/145 [==============================] - 38s 259ms/step - loss: 1.1258 - accuracy: 0.5874 - val_loss: 1.5849 - val_accuracy: 0.3591\n",
      "Epoch 15/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.1143 - accuracy: 0.5986\n",
      "Epoch 15: val_loss did not improve from 1.52164\n",
      "145/145 [==============================] - 35s 241ms/step - loss: 1.1143 - accuracy: 0.5986 - val_loss: 1.5315 - val_accuracy: 0.3529\n",
      "Epoch 16/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.0941 - accuracy: 0.5969\n",
      "Epoch 16: val_loss did not improve from 1.52164\n",
      "145/145 [==============================] - 35s 239ms/step - loss: 1.0941 - accuracy: 0.5969 - val_loss: 1.5525 - val_accuracy: 0.3522\n",
      "Epoch 17/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.0716 - accuracy: 0.6155\n",
      "Epoch 17: val_loss improved from 1.52164 to 1.48302, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 38s 257ms/step - loss: 1.0716 - accuracy: 0.6155 - val_loss: 1.4830 - val_accuracy: 0.4081\n",
      "Epoch 18/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.0645 - accuracy: 0.6205\n",
      "Epoch 18: val_loss did not improve from 1.48302\n",
      "145/145 [==============================] - 34s 234ms/step - loss: 1.0645 - accuracy: 0.6205 - val_loss: 1.5003 - val_accuracy: 0.3729\n",
      "Epoch 19/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.0482 - accuracy: 0.6226\n",
      "Epoch 19: val_loss improved from 1.48302 to 1.48072, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 37s 256ms/step - loss: 1.0482 - accuracy: 0.6226 - val_loss: 1.4807 - val_accuracy: 0.3902\n",
      "Epoch 20/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.0382 - accuracy: 0.6342\n",
      "Epoch 20: val_loss did not improve from 1.48072\n",
      "145/145 [==============================] - 34s 233ms/step - loss: 1.0382 - accuracy: 0.6342 - val_loss: 1.5790 - val_accuracy: 0.3743\n",
      "Epoch 21/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.0262 - accuracy: 0.6414\n",
      "Epoch 21: val_loss improved from 1.48072 to 1.44594, saving model to blindness.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: blindness.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 37s 257ms/step - loss: 1.0262 - accuracy: 0.6414 - val_loss: 1.4459 - val_accuracy: 0.4019\n",
      "Epoch 22/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 1.0108 - accuracy: 0.6462\n",
      "Epoch 22: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 34s 233ms/step - loss: 1.0108 - accuracy: 0.6462 - val_loss: 1.5179 - val_accuracy: 0.4081\n",
      "Epoch 23/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.9924 - accuracy: 0.6518\n",
      "Epoch 23: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 35s 238ms/step - loss: 0.9924 - accuracy: 0.6518 - val_loss: 1.5097 - val_accuracy: 0.3840\n",
      "Epoch 24/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.9781 - accuracy: 0.6626\n",
      "Epoch 24: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 34s 234ms/step - loss: 0.9781 - accuracy: 0.6626 - val_loss: 1.5688 - val_accuracy: 0.4061\n",
      "Epoch 25/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.9805 - accuracy: 0.6664\n",
      "Epoch 25: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 35s 241ms/step - loss: 0.9805 - accuracy: 0.6664 - val_loss: 1.4724 - val_accuracy: 0.3798\n",
      "Epoch 26/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.9567 - accuracy: 0.6770\n",
      "Epoch 26: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 35s 240ms/step - loss: 0.9567 - accuracy: 0.6770 - val_loss: 1.5633 - val_accuracy: 0.4075\n",
      "Epoch 27/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.9461 - accuracy: 0.6790\n",
      "Epoch 27: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 35s 243ms/step - loss: 0.9461 - accuracy: 0.6790 - val_loss: 1.5355 - val_accuracy: 0.3798\n",
      "Epoch 28/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.9369 - accuracy: 0.6932\n",
      "Epoch 28: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 35s 243ms/step - loss: 0.9369 - accuracy: 0.6932 - val_loss: 1.5739 - val_accuracy: 0.3923\n",
      "Epoch 29/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.9228 - accuracy: 0.6987\n",
      "Epoch 29: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 35s 239ms/step - loss: 0.9228 - accuracy: 0.6987 - val_loss: 1.5034 - val_accuracy: 0.4378\n",
      "Epoch 30/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.9099 - accuracy: 0.7015\n",
      "Epoch 30: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 35s 242ms/step - loss: 0.9099 - accuracy: 0.7015 - val_loss: 1.5414 - val_accuracy: 0.4206\n",
      "Epoch 31/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.8923 - accuracy: 0.7094\n",
      "Epoch 31: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 35s 241ms/step - loss: 0.8923 - accuracy: 0.7094 - val_loss: 1.5746 - val_accuracy: 0.4006\n",
      "Epoch 32/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.8874 - accuracy: 0.7156\n",
      "Epoch 32: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 40s 276ms/step - loss: 0.8874 - accuracy: 0.7156 - val_loss: 1.7016 - val_accuracy: 0.3860\n",
      "Epoch 33/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.8774 - accuracy: 0.7135\n",
      "Epoch 33: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 34s 234ms/step - loss: 0.8774 - accuracy: 0.7135 - val_loss: 1.5767 - val_accuracy: 0.3971\n",
      "Epoch 34/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.8625 - accuracy: 0.7287\n",
      "Epoch 34: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 41s 282ms/step - loss: 0.8625 - accuracy: 0.7287 - val_loss: 1.6626 - val_accuracy: 0.3653\n",
      "Epoch 35/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.8408 - accuracy: 0.7356\n",
      "Epoch 35: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 36s 243ms/step - loss: 0.8408 - accuracy: 0.7356 - val_loss: 1.6668 - val_accuracy: 0.4006\n",
      "Epoch 36/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.8392 - accuracy: 0.7465\n",
      "Epoch 36: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 42s 291ms/step - loss: 0.8392 - accuracy: 0.7465 - val_loss: 1.6558 - val_accuracy: 0.4137\n",
      "Epoch 37/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.8357 - accuracy: 0.7450\n",
      "Epoch 37: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 34s 233ms/step - loss: 0.8357 - accuracy: 0.7450 - val_loss: 1.6739 - val_accuracy: 0.3992\n",
      "Epoch 38/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.7922 - accuracy: 0.7639\n",
      "Epoch 38: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 33s 224ms/step - loss: 0.7922 - accuracy: 0.7639 - val_loss: 1.8884 - val_accuracy: 0.3798\n",
      "Epoch 39/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.8125 - accuracy: 0.7519\n",
      "Epoch 39: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 33s 224ms/step - loss: 0.8125 - accuracy: 0.7519 - val_loss: 1.6301 - val_accuracy: 0.4157\n",
      "Epoch 40/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.7856 - accuracy: 0.7582\n",
      "Epoch 40: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 33s 225ms/step - loss: 0.7856 - accuracy: 0.7582 - val_loss: 1.6607 - val_accuracy: 0.4206\n",
      "Epoch 41/50\n",
      "145/145 [==============================] - ETA: 0s - loss: 0.7727 - accuracy: 0.7762\n",
      "Epoch 41: val_loss did not improve from 1.44594\n",
      "145/145 [==============================] - 32s 220ms/step - loss: 0.7727 - accuracy: 0.7762 - val_loss: 1.7630 - val_accuracy: 0.3812\n",
      "Epoch 41: early stopping\n"
     ]
    }
   ],
   "source": [
    "model22=model.fit_generator(generator=train_gen,              \n",
    "                                    steps_per_epoch=len(train_gen),\n",
    "                                    validation_data=test_gen,                    \n",
    "                                    validation_steps=len(test_gen),\n",
    "                                    epochs=50,\n",
    "                                    callbacks = [es,mc], \n",
    "                                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model22.history['accuracy'])\n",
    "plt.plot(model22.history['val_accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
